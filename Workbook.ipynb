{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the topics based on questions asked by Amazon customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is topic modelling?\n",
    "It is an unsupervised approach used for finding and observing the bunch of words (called Topics) in large clusters of texts.\n",
    "\n",
    "### Algorithm used:\n",
    "There are many algorithms available for Topic Modelling such as LSA (Latent Semantic Analysis), NMF (Non-Matrix Factorization) and LDA (Latent Dirichlet Algorithm). I'm using LDA as it provides more accurate results and scales well for large text corpuses.\n",
    "\n",
    "### A brief intro to LDA algorithm:\n",
    "LDA is a matrix factorization technique which assumes sentences are made up of mixture of words and tries to figure out what words would create those sentences in the first place. It is based on probabilistic graphical modelling. The algorithm works as follows:<br>\n",
    "1. In the initialization stage, each word is assigned to a random topic.\n",
    "2. Iteratively, the algorithm goes through each word and reassigns the word to a topic taking into consideration:<br>\n",
    "    What’s the probability of the word belonging to a topic?<br>\n",
    "    What’s the probability of the document to be generated by a topic?\n",
    "[Source]('https://nlpforhackers.io/topic-modeling/')\n",
    "\n",
    "### Data Source:\n",
    "http://jmcauley.ucsd.edu/data/amazon/qa/. Click on the Automotive category file to download the json file having the customer queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hold the data in a table\n",
    "import pandas as pd\n",
    "\n",
    "#Read the zip files\n",
    "import gzip\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Data Preprocessing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "#For visualizing the LDA topics\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Import the data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "data = getDF('qa_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Extract the text column and assign it to an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_name):\n",
    "    data_text = data[['question']]\n",
    "    data_text['index'] = data_text.index\n",
    "    return data_text\n",
    "documents=prepare_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the most useful length to get?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are these cables made of copper or aluminum?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I bought the Red Extra Heavy Duty. Is that too...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi, Being 20ft 4gauge how heavy is this?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do these cables come with a bag?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  index\n",
       "0             What is the most useful length to get?      0\n",
       "1       Are these cables made of copper or aluminum?      1\n",
       "2  I bought the Red Extra Heavy Duty. Is that too...      2\n",
       "3           Hi, Being 20ft 4gauge how heavy is this?      3\n",
       "4                   Do these cables come with a bag?      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Reducing the words to its common  base form\n",
    "Now, we need to reduce the words which are in its plural or derivational forms into its common base form. This will help us collect the similar words from all the corpuses. Now the question is how do we do that? Here, stemming and lemmatization comes to the rescue.<br><br>\n",
    "**What is Stemming?** <br>\n",
    "The process of chopping off the ends of words, often includes the removal of derivational fixes. E.g. cats -> cat<br>\n",
    "There are two types of stemmer: SnowballStemmer and PorterStemmer. We will be using SnowballStemmer as it gives a more meaningful result compared to PorterStemmer().<br>\n",
    "\n",
    "**What is Lemmatization?** <br>\n",
    "The process of removal of inflectional endings only and returning the base or dictionary form of a word, which is known as lemma. E.g. running -> run.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to remove the stopwords and words whose length is less than or equal to three as they won't add much value to our model. We will implement it using gensim library as it already has a tokenization function and built-in library of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_shortwords(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['It', 'does', 'not', 'state', 'what', 'RPM', 'this', 'motor', 'is,', 'does', 'anyone', 'know?']\n",
      "\n",
      "\n",
      " Tokenized and lemmatized document: \n",
      "['state', 'motor', 'know']\n"
     ]
    }
   ],
   "source": [
    "sample = documents[documents['index'] == 25].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n Tokenized and lemmatized document: ')\n",
    "print(remove_stopwords_and_shortwords(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the same on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['question'].map(remove_stopwords_and_shortwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the processed docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [use, length]\n",
       "1           [cabl, copper, aluminum]\n",
       "2    [buy, extra, heavi, duti, size]\n",
       "3                      [gaug, heavi]\n",
       "4                       [cabl, come]\n",
       "5        [wire, pair, surpris, wire]\n",
       "6                       [amp, handl]\n",
       "7              [cabl, boost, school]\n",
       "8                      [use, length]\n",
       "9           [cabl, copper, aluminum]\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Build a dictionary of words present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 14469 words in the dictionary.\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print('There are total '+ str(len(dictionary))+' words in the dictionary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 length\n",
      "1 use\n",
      "2 aluminum\n",
      "3 cabl\n",
      "4 copper\n",
      "5 buy\n",
      "6 duti\n",
      "7 extra\n",
      "8 heavi\n",
      "9 size\n",
      "10 gaug\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index, word in dictionary.iteritems():\n",
    "    print(index, word)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Filter the dictionary based on frequency of words. \n",
    "Here, I have ignored the words which have appeared in less than 4 queries and the ones who are present in at most 50% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words in the dictionary after filtering: 4705\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=4, no_above=0.5)\n",
    "print('No. of words in the dictionary after filtering: '+ str(len(dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Create a list having the index value and the frequency of the word for every question in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(8, 1), (10, 1)],\n",
       " [(3, 1), (11, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"length\") appears 1 time.\n",
      "Word 1 (\"use\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_sample = bow_corpus[0]\n",
    "\n",
    "for i in range(len(bow_doc_sample)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_sample[i][0], \n",
    "                                                     dictionary[bow_doc_sample[i][0]], \n",
    "                                                     bow_doc_sample[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Implement the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=5, id2word=dictionary, passes=5,minimum_probability=0.01, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.041*\"light\" + 0.036*\"work\" + 0.036*\"bulb\" + 0.032*\"toyota\" + 0.019*\"dodg\" + 0.018*\"inch\" + 0.015*\"dimens\"\n",
      "Topic: 1 \n",
      "Words: 0.036*\"instal\" + 0.031*\"come\" + 0.025*\"mount\" + 0.025*\"need\" + 0.024*\"wire\" + 0.018*\"light\" + 0.015*\"work\"\n",
      "Topic: 2 \n",
      "Words: 0.031*\"size\" + 0.031*\"tire\" + 0.024*\"need\" + 0.018*\"color\" + 0.018*\"order\" + 0.017*\"black\" + 0.016*\"know\"\n",
      "Topic: 3 \n",
      "Words: 0.091*\"work\" + 0.036*\"seat\" + 0.033*\"model\" + 0.025*\"honda\" + 0.021*\"ford\" + 0.020*\"chevi\" + 0.019*\"cover\"\n",
      "Topic: 4 \n",
      "Words: 0.030*\"batteri\" + 0.027*\"say\" + 0.019*\"unit\" + 0.017*\"plug\" + 0.015*\"light\" + 0.015*\"power\" + 0.015*\"charg\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1,num_words=7):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Visualize the topics\n",
    "I have used pyLDAvis library which is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.[Source]('https://github.com/bmabey/pyLDAvis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el103642041426153544403529735\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el103642041426153544403529735_data = {\"mdsDat\": {\"x\": [-0.11576398396226788, -0.2227472357104045, -0.13412902460288026, 0.3224171241212757, 0.15022312015427694], \"y\": [0.17417914946494492, 0.007071917085564132, -0.26431720470396686, -0.1521083223263535, 0.23517446047981158], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [24.26449966430664, 20.574907302856445, 18.964204788208008, 18.642494201660156, 17.55389404296875]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [10520.0, 3454.0, 2600.0, 2454.0, 2381.0, 2326.0, 2212.0, 3453.489013671875, 2365.15380859375, 2310.8984375, 1037.6812744140625, 967.5553588867188, 841.7793579101562, 810.1751708984375, 2976.34619140625, 1266.1300048828125, 2341.2880859375, 1668.3514404296875, 1385.43603515625, 2383.27685546875, 2166.9111328125, 1386.432373046875, 1173.1702880859375, 966.3255004882812, 763.5396118164062, 742.3934326171875, 1560.68896484375, 1229.3387451171875, 1236.37109375, 2325.22607421875, 2313.53173828125, 1371.092041015625, 1301.977783203125, 1265.787353515625, 993.3165283203125, 946.4297485351562, 1773.4901123046875, 1112.9830322265625, 1172.5579833984375, 2599.744140625, 2380.328369140625, 1488.36279296875, 1140.4481201171875, 989.1632080078125, 953.5733642578125, 893.402587890625, 6674.41357421875, 1851.8402099609375, 1501.23193359375, 1388.9261474609375, 2453.61328125, 2211.643798828125, 1281.7730712890625, 1209.17578125, 1023.7332153320312, 1001.75244140625, 963.8869018554688, 2846.49560546875, 2460.094482421875], \"Term\": [\"work\", \"instal\", \"seat\", \"bulb\", \"model\", \"size\", \"toyota\", \"instal\", \"mount\", \"wire\", \"switch\", \"remov\", \"bracket\", \"remot\", \"come\", \"includ\", \"need\", \"light\", \"work\", \"batteri\", \"say\", \"plug\", \"charg\", \"accord\", \"price\", \"water\", \"unit\", \"power\", \"light\", \"size\", \"tire\", \"color\", \"order\", \"black\", \"purchas\", \"wrangler\", \"need\", \"door\", \"know\", \"seat\", \"model\", \"chevi\", \"nissan\", \"mirror\", \"mat\", \"engin\", \"work\", \"honda\", \"ford\", \"cover\", \"bulb\", \"toyota\", \"dodg\", \"inch\", \"dimens\", \"ship\", \"hole\", \"light\", \"work\"], \"Total\": [10520.0, 3454.0, 2600.0, 2454.0, 2381.0, 2326.0, 2212.0, 3454.304931640625, 2365.96875, 2311.7119140625, 1038.498046875, 968.3886108398438, 842.593017578125, 810.9887084960938, 4805.89501953125, 1649.6212158203125, 5533.953125, 5751.640625, 10520.3662109375, 2384.0810546875, 2167.716796875, 1387.240234375, 1173.9742431640625, 967.1323852539062, 764.34228515625, 743.1995239257812, 1564.04638671875, 1558.27490234375, 5751.640625, 2326.017578125, 2314.32470703125, 1371.88330078125, 1302.7735595703125, 1266.580810546875, 994.1138916015625, 947.2192993164062, 5533.953125, 1576.5498046875, 2585.602294921875, 2600.552001953125, 2381.13818359375, 1489.1683349609375, 1141.2542724609375, 989.970458984375, 954.381103515625, 894.210205078125, 10520.3662109375, 2645.082275390625, 1916.2777099609375, 2522.415771484375, 2454.429443359375, 2212.456298828125, 1282.584716796875, 1209.9920654296875, 1024.546875, 1002.5665893554688, 964.7037353515625, 5751.640625, 10520.3662109375], \"loglift\": [7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4158999919891357, 1.4157999753952026, 1.4157999753952026, 1.4154000282287598, 1.4153000116348267, 1.4151999950408936, 1.4151999950408936, 0.9369999766349792, 1.1516000032424927, 0.5559999942779541, 0.1784999966621399, -0.6111000180244446, 1.5808000564575195, 1.5807000398635864, 1.5805000066757202, 1.580399990081787, 1.580299973487854, 1.5800000429153442, 1.5800000429153442, 1.5788999795913696, 1.343999981880188, 0.043800000101327896, 1.6622999906539917, 1.6622999906539917, 1.6619999408721924, 1.6619999408721924, 1.6619999408721924, 1.6618000268936157, 1.6618000268936157, 0.5246999859809875, 1.3143999576568604, 0.8718000054359436, 1.6793999671936035, 1.6793999671936035, 1.6792000532150269, 1.6790000200271606, 1.6789000034332275, 1.6789000034332275, 1.6787999868392944, 1.2246999740600586, 1.323199987411499, 1.4356000423431396, 1.0829999446868896, 1.7395999431610107, 1.7395000457763672, 1.739300012588501, 1.7391999959945679, 1.7390999794006348, 1.7390999794006348, 1.7389999628067017, 1.0364999771118164, 0.28679999709129333], \"logprob\": [7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.316200017929077, -3.6947999000549316, -3.7179999351501465, -4.518599987030029, -4.588600158691406, -4.727799892425537, -4.76609992980957, -3.464900016784668, -4.3196001052856445, -3.704900026321411, -4.043799877166748, -4.229599952697754, -3.522200107574463, -3.6173999309539795, -4.063899993896484, -4.230999946594238, -4.424900054931641, -4.6605000495910645, -4.688499927520752, -3.945499897003174, -4.184199810028076, -4.178500175476074, -3.4653000831604004, -3.470400094985962, -3.993499994277954, -4.045300006866455, -4.073500156402588, -4.315899848937988, -4.364200115203857, -3.7362000942230225, -4.202099800109863, -4.150000095367432, -3.3366000652313232, -3.424799919128418, -3.894399881362915, -4.160600185394287, -4.3028998374938965, -4.339600086212158, -4.404799938201904, -2.3938000202178955, -3.6758999824523926, -3.8857998847961426, -3.9635000228881836, -3.3343000411987305, -3.4381000995635986, -3.983599901199341, -4.041900157928467, -4.208399772644043, -4.230100154876709, -4.268700122833252, -3.185800075531006, -3.331700086593628]}, \"token.table\": {\"Topic\": [2, 2, 3, 1, 5, 2, 4, 3, 1, 2, 3, 1, 2, 3, 4, 5, 5, 5, 3, 4, 4, 3, 4, 5, 2, 4, 5, 1, 2, 1, 1, 2, 3, 4, 5, 1, 2, 5, 4, 4, 4, 1, 1, 2, 3, 5, 4, 3, 2, 1, 2, 4, 2, 3, 1, 1, 2, 4, 5, 3, 1, 3, 5, 2, 3, 2, 1, 1, 4, 5, 3], \"Freq\": [0.9988291263580322, 0.9995465278625488, 0.9995414614677429, 0.9992961883544922, 0.9998250603675842, 0.9991701245307922, 0.9992154240608215, 0.9993561506271362, 0.6192395091056824, 0.20287583768367767, 0.17790651321411133, 0.1125904768705368, 0.000396445335354656, 0.22319872677326202, 0.5506625771522522, 0.11298692226409912, 0.9994662404060364, 0.999544084072113, 0.7059720158576965, 0.29367926716804504, 0.99864661693573, 0.2160438448190689, 0.78328937292099, 0.999270498752594, 0.29980164766311646, 0.7001672387123108, 0.9991800785064697, 0.767448902130127, 0.23217451572418213, 0.9996222257614136, 0.08740709722042084, 0.21348990499973297, 0.4536660611629486, 0.17868177592754364, 0.06690897792577744, 0.2900042235851288, 0.21489520370960236, 0.4948153495788574, 0.9996007084846497, 0.9990196824073792, 0.9995219707489014, 0.9995905756950378, 0.42302489280700684, 0.141490176320076, 0.32038581371307373, 0.11492688953876495, 0.9989009499549866, 0.9994062185287476, 0.9991059899330139, 0.019252058118581772, 0.7886926531791687, 0.19123712182044983, 0.9995521903038025, 0.9988794922828674, 0.9987808465957642, 0.9995986819267273, 0.9996693134307861, 0.9997877478599548, 0.9994348883628845, 0.9995625019073486, 0.9995204210281372, 0.9998596906661987, 0.9997937679290771, 0.9980522394180298, 0.0019181016832590103, 0.998386025428772, 0.9996920228004456, 0.13164940476417542, 0.6343885660171509, 0.23383216559886932, 0.9987127780914307], \"Term\": [\"accord\", \"batteri\", \"black\", \"bracket\", \"bulb\", \"charg\", \"chevi\", \"color\", \"come\", \"come\", \"come\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"dimens\", \"dodg\", \"door\", \"door\", \"engin\", \"ford\", \"ford\", \"hole\", \"honda\", \"honda\", \"inch\", \"includ\", \"includ\", \"instal\", \"know\", \"know\", \"know\", \"know\", \"know\", \"light\", \"light\", \"light\", \"mat\", \"mirror\", \"model\", \"mount\", \"need\", \"need\", \"need\", \"need\", \"nissan\", \"order\", \"plug\", \"power\", \"power\", \"power\", \"price\", \"purchas\", \"remot\", \"remov\", \"say\", \"seat\", \"ship\", \"size\", \"switch\", \"tire\", \"toyota\", \"unit\", \"unit\", \"water\", \"wire\", \"work\", \"work\", \"work\", \"wrangler\"]}, \"R\": 7, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 5, 3, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el103642041426153544403529735\", ldavis_el103642041426153544403529735_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el103642041426153544403529735\", ldavis_el103642041426153544403529735_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el103642041426153544403529735\", ldavis_el103642041426153544403529735_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1     -0.115764  0.174179       1        1  24.264500\n",
       "4     -0.222747  0.007072       2        1  20.574907\n",
       "2     -0.134129 -0.264317       3        1  18.964205\n",
       "3      0.322417 -0.152108       4        1  18.642494\n",
       "0      0.150223  0.235174       5        1  17.553894, topic_info=     Category          Freq      Term         Total  loglift  logprob\n",
       "term                                                                 \n",
       "79    Default  10520.000000      work  10520.000000   7.0000   7.0000\n",
       "148   Default   3454.000000    instal   3454.000000   6.0000   6.0000\n",
       "674   Default   2600.000000      seat   2600.000000   5.0000   5.0000\n",
       "1464  Default   2454.000000      bulb   2454.000000   4.0000   4.0000\n",
       "26    Default   2381.000000     model   2381.000000   3.0000   3.0000\n",
       "9     Default   2326.000000      size   2326.000000   2.0000   2.0000\n",
       "344   Default   2212.000000    toyota   2212.000000   1.0000   1.0000\n",
       "148    Topic1   3453.489014    instal   3454.304932   1.4159  -3.3162\n",
       "150    Topic1   2365.153809     mount   2365.968750   1.4158  -3.6948\n",
       "14     Topic1   2310.898438      wire   2311.711914   1.4158  -3.7180\n",
       "81     Topic1   1037.681274    switch   1038.498047   1.4154  -4.5186\n",
       "503    Topic1    967.555359     remov    968.388611   1.4153  -4.5886\n",
       "256    Topic1    841.779358   bracket    842.593018   1.4152  -4.7278\n",
       "221    Topic1    810.175171     remot    810.988708   1.4152  -4.7661\n",
       "11     Topic1   2976.346191      come   4805.895020   0.9370  -3.4649\n",
       "166    Topic1   1266.130005    includ   1649.621216   1.1516  -4.3196\n",
       "198    Topic1   2341.288086      need   5533.953125   0.5560  -3.7049\n",
       "84     Topic1   1668.351440     light   5751.640625   0.1785  -4.0438\n",
       "79     Topic1   1385.436035      work  10520.366211  -0.6111  -4.2296\n",
       "44     Topic2   2383.276855   batteri   2384.081055   1.5808  -3.5222\n",
       "36     Topic2   2166.911133       say   2167.716797   1.5807  -3.6174\n",
       "157    Topic2   1386.432373      plug   1387.240234   1.5805  -4.0639\n",
       "49     Topic2   1173.170288     charg   1173.974243   1.5804  -4.2310\n",
       "309    Topic2    966.325500    accord    967.132385   1.5803  -4.4249\n",
       "386    Topic2    763.539612     price    764.342285   1.5800  -4.6605\n",
       "156    Topic2    742.393433     water    743.199524   1.5800  -4.6885\n",
       "154    Topic2   1560.688965      unit   1564.046387   1.5789  -3.9455\n",
       "39     Topic2   1229.338745     power   1558.274902   1.3440  -4.1842\n",
       "84     Topic2   1236.371094     light   5751.640625   0.0438  -4.1785\n",
       "9      Topic3   2325.226074      size   2326.017578   1.6623  -3.4653\n",
       "339    Topic3   2313.531738      tire   2314.324707   1.6623  -3.4704\n",
       "264    Topic3   1371.092041     color   1371.883301   1.6620  -3.9935\n",
       "384    Topic3   1301.977783     order   1302.773560   1.6620  -4.0453\n",
       "218    Topic3   1265.787354     black   1266.580811   1.6620  -4.0735\n",
       "52     Topic3    993.316528   purchas    994.113892   1.6618  -4.3159\n",
       "340    Topic3    946.429749  wrangler    947.219299   1.6618  -4.3642\n",
       "198    Topic3   1773.490112      need   5533.953125   0.5247  -3.7362\n",
       "258    Topic3   1112.983032      door   1576.549805   1.3144  -4.2021\n",
       "21     Topic3   1172.557983      know   2585.602295   0.8718  -4.1500\n",
       "674    Topic4   2599.744141      seat   2600.552002   1.6794  -3.3366\n",
       "26     Topic4   2380.328369     model   2381.138184   1.6794  -3.4248\n",
       "743    Topic4   1488.362793     chevi   1489.168335   1.6792  -3.8944\n",
       "467    Topic4   1140.448120    nissan   1141.254272   1.6790  -4.1606\n",
       "757    Topic4    989.163208    mirror    989.970459   1.6789  -4.3029\n",
       "1341   Topic4    953.573364       mat    954.381104   1.6789  -4.3396\n",
       "213    Topic4    893.402588     engin    894.210205   1.6788  -4.4048\n",
       "79     Topic4   6674.413574      work  10520.366211   1.2247  -2.3938\n",
       "310    Topic4   1851.840210     honda   2645.082275   1.3232  -3.6759\n",
       "301    Topic4   1501.231934      ford   1916.277710   1.4356  -3.8858\n",
       "493    Topic4   1388.926147     cover   2522.415771   1.0830  -3.9635\n",
       "1464   Topic5   2453.613281      bulb   2454.429443   1.7396  -3.3343\n",
       "344    Topic5   2211.643799    toyota   2212.456299   1.7395  -3.4381\n",
       "400    Topic5   1281.773071      dodg   1282.584717   1.7393  -3.9836\n",
       "89     Topic5   1209.175781      inch   1209.992065   1.7392  -4.0419\n",
       "53     Topic5   1023.733215    dimens   1024.546875   1.7391  -4.2084\n",
       "385    Topic5   1001.752441      ship   1002.566589   1.7391  -4.2301\n",
       "92     Topic5    963.886902      hole    964.703735   1.7390  -4.2687\n",
       "84     Topic5   2846.495605     light   5751.640625   1.0365  -3.1858\n",
       "79     Topic5   2460.094482      work  10520.366211   0.2868  -3.3317, token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "309       2  0.998829    accord\n",
       "44        2  0.999547   batteri\n",
       "218       3  0.999541     black\n",
       "256       1  0.999296   bracket\n",
       "1464      5  0.999825      bulb\n",
       "49        2  0.999170     charg\n",
       "743       4  0.999215     chevi\n",
       "264       3  0.999356     color\n",
       "11        1  0.619240      come\n",
       "11        2  0.202876      come\n",
       "11        3  0.177907      come\n",
       "493       1  0.112590     cover\n",
       "493       2  0.000396     cover\n",
       "493       3  0.223199     cover\n",
       "493       4  0.550663     cover\n",
       "493       5  0.112987     cover\n",
       "53        5  0.999466    dimens\n",
       "400       5  0.999544      dodg\n",
       "258       3  0.705972      door\n",
       "258       4  0.293679      door\n",
       "213       4  0.998647     engin\n",
       "301       3  0.216044      ford\n",
       "301       4  0.783289      ford\n",
       "92        5  0.999270      hole\n",
       "310       2  0.299802     honda\n",
       "310       4  0.700167     honda\n",
       "89        5  0.999180      inch\n",
       "166       1  0.767449    includ\n",
       "166       2  0.232175    includ\n",
       "148       1  0.999622    instal\n",
       "...     ...       ...       ...\n",
       "150       1  0.999591     mount\n",
       "198       1  0.423025      need\n",
       "198       2  0.141490      need\n",
       "198       3  0.320386      need\n",
       "198       5  0.114927      need\n",
       "467       4  0.998901    nissan\n",
       "384       3  0.999406     order\n",
       "157       2  0.999106      plug\n",
       "39        1  0.019252     power\n",
       "39        2  0.788693     power\n",
       "39        4  0.191237     power\n",
       "386       2  0.999552     price\n",
       "52        3  0.998879   purchas\n",
       "221       1  0.998781     remot\n",
       "503       1  0.999599     remov\n",
       "36        2  0.999669       say\n",
       "674       4  0.999788      seat\n",
       "385       5  0.999435      ship\n",
       "9         3  0.999563      size\n",
       "81        1  0.999520    switch\n",
       "339       3  0.999860      tire\n",
       "344       5  0.999794    toyota\n",
       "154       2  0.998052      unit\n",
       "154       3  0.001918      unit\n",
       "156       2  0.998386     water\n",
       "14        1  0.999692      wire\n",
       "79        1  0.131649      work\n",
       "79        4  0.634389      work\n",
       "79        5  0.233832      work\n",
       "340       3  0.998713  wrangler\n",
       "\n",
       "[71 rows x 3 columns], R=7, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 5, 3, 4, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis1=pyLDAvis.gensim.prepare(lda_model,bow_corpus,dictionary, R=7)\n",
    "vis1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: Intepretation of topics obtained:\n",
    "\n",
    "Now, let's try to identify the category of questions asked from the words provided for each topic:\n",
    "1. Topic 1 (install, come, mount, wire, need, light, work) - **Installation**\n",
    "2. Topic 2 (battery, say, unit, plug, light, power, charge) - **Battery & Charging**\n",
    "3. Topic 3 (size, tire, need, color, order, black, know) - **Tire**\n",
    "4. Topic 4 (work, seat, honda, model, chevi, ford, cover) - **Model specific**\n",
    "5. Topic 5 (light, bulb, work, toyota, dodge, inch, dimension) - **Cannot deduce a valid topic** <br>\n",
    "\n",
    "There are cases when the some of the topics obtained don't make much sense. One way to fix it would be to play around the hyperparameters in order to improve the results or sometimes its okay to just ignore them. Here, we can see that words in Topic 5 are not making much sense for categorizing it into specific category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
